"7_oth"
)
streets$type_num <- as.numeric(factor(streets$type, levels = type_levels))
# Rasterize street data
street_raster <- rasterize(
vect(streets),
rast_grid,
field = "type_num",
fun = "min",
touches = FALSE
)
# Convert to set of points (this is slow)
street_points <- as.data.frame(street_raster, xy = TRUE, na.rm = TRUE) %>%
rename(lon = x, lat = y, type_num = last_col()) %>%
mutate(
street_type = type_levels[type_num],
point_id = row_number()
) %>%
select(point_id, lon, lat, street_type)
mem.maxVSize(v = Inf)
# Convert to set of points (this is slow)
street_points <- as.data.frame(street_raster, xy = TRUE, na.rm = TRUE) %>%
rename(lon = x, lat = y, type_num = last_col()) %>%
mutate(
street_type = type_levels[type_num],
point_id = row_number()
) %>%
select(point_id, lon, lat, street_type)
library(arrow)
street_points = read_parquet("../data/street_points.parquet")
View(street_points)
# Function to join nearest weather station data
add_loc <- function(data, stations, coords_cols = c("lon", "lat")) {
# Convert coordinates to matrices
coords_input <- as.matrix(data[, coords_cols])
coords_stations <- as.matrix(stations[, c("longitude", "latitude")])
# Find nearest station index
nearest_idx <- nn2(coords_stations, coords_input, k = 1)$nn.idx[, 1]
# Assign location_id and
data <- data %>%
mutate(location_id = stations$location_id[nearest_idx])
return(data)
}
street_points <- add_loc(street_points, id_lookup)
library(RANN)
add_loc <- function(data, stations, coords_cols = c("lon", "lat")) {
# Convert coordinates to matrices
coords_input <- as.matrix(data[, coords_cols])
coords_stations <- as.matrix(stations[, c("longitude", "latitude")])
# Find nearest station index
nearest_idx <- nn2(coords_stations, coords_input, k = 1)$nn.idx[, 1]
# Assign location_id and
data <- data %>%
mutate(location_id = stations$location_id[nearest_idx])
return(data)
}
street_points <- add_loc(street_points, id_lookup)
View(street_points)
library(ggplot2)
ggplot(street_points, aes(x = lon, y = lat, color = factor(location_id))) +
geom_point(size = 1) +
coord_equal() +
theme_minimal()
View(street_points)
1+2
# Save street points
write_parquet(street_points, "../data/street_points.parquet")
library(ggplot2)
ggplot(street_points, aes(x = lon, y = lat, color = factor(location_id))) +
geom_point(size = 1) +
coord_equal() +
theme_minimal()
# ==============================================================================
# Traffic Crash Analysis Data Preprocessing Pipeline
# ==============================================================================
# Purpose: Prepare ML-ready dataset combining crash records with weather data
#          and generate negative samples for crash prediction modeling
#
# Input Files:
#   - weather.csv: Hourly weather observations
#   - id_lookup.csv: Weather station locations
#   - crashes.csv: Traffic crash records
#   - ../data/street_points.parquet: Street network sample points
#
# Output:
#   - ../data/ml_input_data.parquet: ML-ready dataset with features
# ==============================================================================
# ================= LIBRARIES =================
library(data.table)
library(tidyverse)
library(lubridate)
library(suncalc)
library(sf)
library(RANN)
library(geosphere)
library(slider)
library(janitor)
library(fastDummies)
library(arrow)
# ================= WEATHER DATA PROCESSING =================
# Read weather CSV and clean column names
weather <- fread(
"weather.csv",
encoding = "UTF-8",
na.strings = c("", "NA", "NULL"),
fill = TRUE,
data.table = FALSE
) %>%
clean_names()
# Read weather station lookup table and clean column names
id_lookup <- fread(
"id_lookup.csv",
encoding = "UTF-8",
na.strings = c("", "NA", "NULL"),
fill = TRUE,
data.table = FALSE
) %>%
clean_names()
# Extract datetime components and set timezone
weather <- weather %>%
mutate(datetime = ymd_hm(time)) %>%
mutate(datetime = with_tz(datetime, tzone = "America/Los_Angeles")) %>%
select(-time)
# Compute rolling 3-hour cumulative rain per station
weather <- weather %>%
arrange(location_id, datetime) %>%
group_by(location_id) %>%
mutate(
rain_3hr = slide_index_dbl(
.x = rain_inch,
.i = datetime,
.f = ~ sum(.x),
.before = dhours(3),
.complete = FALSE
)
) %>%
ungroup()
# Add daylight flag
mean_lat = mean(id_lookup$latitude)
mean_lon = mean(id_lookup$longitude)
weather <- weather %>%
mutate(
date_local = as.Date(datetime, tz = "America/Los_Angeles"),
sun_times = getSunlightTimes(
date = date_local,
lat  = mean_lat,
lon  = mean_lon,
tz   = "America/Los_Angeles"
),
sunrise = sun_times$sunrise,
sunset  = sun_times$sunset,
is_day  = as.numeric(datetime >= sunrise & datetime <= sunset)
) %>%
select(-sun_times, -date_local, -sunrise, -sunset)
# Drop some unnecessary columns and simplify names
weather <- weather %>%
select(-weather_code_wmo_code,
-wind_direction_10m,
-apparent_temperature_f,
-dew_point_2m_f,
-surface_pressure_h_pa) %>%
rename(
temp = temperature_2m_f,
humidity = relative_humidity_2m_percent,
rain = rain_inch,
snow = snowfall_inch,
snow_depth = snow_depth_ft,
cloud_cover = cloud_cover_percent,
wind_speed = wind_speed_10m_mp_h,
wind_gusts = wind_gusts_10m_mp_h
)
# ================= CRASH DATA PROCESSING =================
# Read crash data and clean column names
crashes <- fread(
"crashes.csv",
encoding = "UTF-8",
na.strings = c("", "NA", "NULL"),
data.table = FALSE
) %>%
clean_names()
# Filter for Portland urban area, remove unlocatable crashes,
# remove crashes with missing time, add nicely formatted datetime
crashes <- crashes %>%
filter(
urb_area_short_nm == "PORTLAND UA",
unloct_flg == 0,
crash_hr_no != 99
) %>%
mutate(
hour_str = sprintf("%02d", crash_hr_no),
datetime_str = paste(crash_dt, paste0(hour_str, ":00")),
datetime = ymd_hm(datetime_str) %>%
force_tz(tzone = "America/Los_Angeles")
) %>%
select(-hour_str, -datetime_str)
# ================= CRASH STREET POINT ASSIGNMENT =================
street_points <- read_parquet("../data/street_points.parquet")
# Function for finding nearest street point
assign_nearest_street <- function(crashes, street_points) {
crash_mat  <- as.matrix(crashes[, c("longtd_dd", "lat_dd")])
street_mat <- as.matrix(street_points[, c("lon", "lat")])
nearest <- nn2(street_mat, crash_mat, k = 1)
idx <- nearest$nn.idx[, 1]
# Append street info to the original crashes data
crashes$street_type <- street_points$street_type[idx]
crashes$lon <- street_points$lon[idx]
crashes$lat <- street_points$lat[idx]
crashes$location_id <- street_points$location_id[idx]
crashes
}
# Get nearest street point
crashes <- assign_nearest_street(crashes, street_points)
# Create positive data points (crashes)
positives <- crashes %>%
mutate(crash_occurred = 1) %>%
select(
datetime,
lat,
lon,
crash_mo_no,
crash_wk_day_cd,
crash_hr_no,
street_type,
crash_occurred,
crash_svrty_short_desc
)  %>%
rename(
month = crash_mo_no,
day = crash_wk_day_cd,
hour = crash_hr_no,
svrty = crash_svrty_short_desc
)
# ================= NEGATIVE SAMPLE GENERATION =================
set.seed(123)
sample_ratio = 10 * nrow(positives) # 10:1 ratio of non-crashes to crashes
num_gen = sample_ratio * 1.001 # Slight oversample in case of overlap
# Set time range
time_start <- as.POSIXct("2019-01-01 00:00:00", tz = "America/Los_Angeles")
time_end   <- as.POSIXct("2024-12-31 15:00:00", tz = "America/Los_Angeles")
# Sample rows from street_points, assign random datetime, remove overlap
negatives <- street_points %>%
sample_n(num_gen, replace = TRUE) %>%
mutate(datetime = sample(
seq(time_start, time_end, by = "hour"),
num_gen, replace = TRUE
)
) %>%
anti_join(positives, by = c("lon", "lat", "datetime")) %>%
sample_n(sample_ratio)
# Compute features and clean up negative samples
negatives <- negatives %>%
mutate(
month = month(datetime),
day = wday(datetime, week_start = 7),
hour = hour(datetime),
crash_occurred = 0,
svrty = NA
) %>%
select(-point_id)
View(positives)
# Function for finding nearest street point
assign_nearest_street <- function(crashes, street_points) {
crash_mat  <- as.matrix(crashes[, c("longtd_dd", "lat_dd")])
street_mat <- as.matrix(street_points[, c("lon", "lat")])
nearest <- nn2(street_mat, crash_mat, k = 1)
idx <- nearest$nn.idx[, 1]
# Append street info to the original crashes data
crashes$street_type <- street_points$street_type[idx]
crashes$lon <- street_points$lon[idx]
crashes$lat <- street_points$lat[idx]
crashes$location_id <- street_points$location_id[idx]
crashes
}
# Get nearest street point
crashes <- assign_nearest_street(crashes, street_points)
# Create positive data points (crashes)
positives <- crashes %>%
mutate(crash_occurred = 1) %>%
select(
datetime,
lat,
lon,
crash_mo_no,
crash_wk_day_cd,
crash_hr_no,
street_type,
crash_occurred,
crash_svrty_short_desc,
location_id
)  %>%
rename(
month = crash_mo_no,
day = crash_wk_day_cd,
hour = crash_hr_no,
svrty = crash_svrty_short_desc
)
library(ggplot2)
ggplot(positives, aes(x = lon, y = lat, color = factor(location_id))) +
geom_point(size = 1) +
coord_equal() +
theme_minimal()
library(data.table)
library(tidyverse)
library(lubridate)
library(suncalc)
library(sf)
library(RANN)
library(geosphere)
library(slider)
library(janitor)
library(fastDummies)
library(arrow)
set.seed(123)
sample_ratio = 10 * nrow(positives) # 10:1 ratio of non-crashes to crashes
num_gen = sample_ratio * 1.001 # Slight oversample in case of overlap
# Set time range
time_start <- as.POSIXct("2019-01-01 00:00:00", tz = "America/Los_Angeles")
time_end   <- as.POSIXct("2024-12-31 15:00:00", tz = "America/Los_Angeles")
# Sample rows from street_points, assign random datetime, remove overlap
negatives <- street_points %>%
sample_n(num_gen, replace = TRUE) %>%
mutate(datetime = sample(
seq(time_start, time_end, by = "hour"),
num_gen, replace = TRUE
)
) %>%
anti_join(positives, by = c("lon", "lat", "datetime")) %>%
sample_n(sample_ratio)
# Compute features and clean up negative samples
negatives <- negatives %>%
mutate(
month = month(datetime),
day = wday(datetime, week_start = 7),
hour = hour(datetime),
crash_occurred = 0,
svrty = NA
) %>%
select(-point_id)
View(negatives)
library(lubridate)
find("wday")
set.seed(123)
sample_ratio = 10 * nrow(positives) # 10:1 ratio of non-crashes to crashes
num_gen = sample_ratio * 1.001 # Slight oversample in case of overlap
# Set time range
time_start <- as.POSIXct("2019-01-01 00:00:00", tz = "America/Los_Angeles")
time_end   <- as.POSIXct("2024-12-31 15:00:00", tz = "America/Los_Angeles")
# Sample rows from street_points, assign random datetime, remove overlap
negatives <- street_points %>%
sample_n(num_gen, replace = TRUE) %>%
mutate(datetime = sample(
seq(time_start, time_end, by = "hour"),
num_gen, replace = TRUE
)
) %>%
anti_join(positives, by = c("lon", "lat", "datetime")) %>%
sample_n(sample_ratio)
# Compute features and clean up negative samples
negatives <- negatives %>%
mutate(
month = month(datetime),
day = lubridate::wday(datetime, week_start = 7),
hour = hour(datetime),
crash_occurred = 0,
svrty = NA
) %>%
select(-point_id)
library(ggplot2)
ggplot(negatives, aes(x = lon, y = lat, color = factor(location_id))) +
geom_point(size = 1) +
coord_equal() +
theme_minimal()
# Combine positives and negatives
ml_input_data <- bind_rows(positives, negatives)
# Add weather data for all points
ml_input_data <- ml_input_data %>%
mutate(location_id = stations$location_id[nearest_idx]) %>%
left_join(weather, by = c("location_id", "datetime")) %>%
select(-location_id) %>%
filter(!is.na(temp))
# Add weather data for all points
ml_input_data <- ml_input_data %>%
mutate(location_id = weather$location_id[nearest_idx]) %>%
left_join(weather, by = c("location_id", "datetime")) %>%
select(-location_id) %>%
filter(!is.na(temp))
# Add weather data for all points
ml_input_data <- ml_input_data %>%
left_join(weather, by = c("location_id", "datetime")) %>%
select(-location_id) %>%
filter(!is.na(temp))
View(ml_input_data)
weather <- weather %>%
mutate(
date_local = as.Date(datetime, tz = "America/Los_Angeles"),
sun_times = getSunlightTimes(
date = date_local,
lat  = mean_lat,
lon  = mean_lon,
tz   = "America/Los_Angeles"
),
sunrise = sun_times$sunrise,
sunset  = sun_times$sunset,
is_day  = as.numeric(datetime >= sunrise & datetime <= sunset)
) %>%
select(-sun_times, -date_local, -sunrise, -sunset)
library(data.table)
library(tidyverse)
library(lubridate)
library(suncalc)
library(sf)
library(RANN)
library(geosphere)
library(slider)
library(janitor)
library(fastDummies)
library(arrow)
weather <- weather %>%
mutate(
date_local = as.Date(datetime, tz = "America/Los_Angeles"),
sun_times = getSunlightTimes(
date = date_local,
lat  = mean_lat,
lon  = mean_lon,
tz   = "America/Los_Angeles"
),
sunrise = sun_times$sunrise,
sunset  = sun_times$sunset,
is_day  = as.numeric(datetime >= sunrise & datetime <= sunset)
) %>%
select(-sun_times, -date_local, -sunrise, -sunset)
# Read weather CSV and clean column names
weather <- fread(
"weather.csv",
encoding = "UTF-8",
na.strings = c("", "NA", "NULL"),
fill = TRUE,
data.table = FALSE
) %>%
clean_names()
# Read weather station lookup table and clean column names
id_lookup <- fread(
"id_lookup.csv",
encoding = "UTF-8",
na.strings = c("", "NA", "NULL"),
fill = TRUE,
data.table = FALSE
) %>%
clean_names()
# Extract datetime components and set timezone
weather <- weather %>%
mutate(datetime = ymd_hm(time)) %>%
mutate(datetime = with_tz(datetime, tzone = "America/Los_Angeles")) %>%
select(-time)
# Compute rolling 3-hour cumulative rain per station
weather <- weather %>%
arrange(location_id, datetime) %>%
group_by(location_id) %>%
mutate(
rain_3hr = slide_index_dbl(
.x = rain_inch,
.i = datetime,
.f = ~ sum(.x),
.before = dhours(3),
.complete = FALSE
)
) %>%
ungroup()
# Add daylight flag
mean_lat = mean(id_lookup$latitude)
mean_lon = mean(id_lookup$longitude)
weather <- weather %>%
mutate(
date_local = as.Date(datetime, tz = "America/Los_Angeles"),
sun_times = getSunlightTimes(
date = date_local,
lat  = mean_lat,
lon  = mean_lon,
tz   = "America/Los_Angeles"
),
sunrise = sun_times$sunrise,
sunset  = sun_times$sunset,
is_day  = as.numeric(datetime >= sunrise & datetime <= sunset)
) %>%
select(-sun_times, -date_local, -sunrise, -sunset)
# Drop some unnecessary columns and simplify names
weather <- weather %>%
select(-weather_code_wmo_code,
-wind_direction_10m,
-apparent_temperature_f,
-dew_point_2m_f,
-surface_pressure_h_pa) %>%
rename(
temp = temperature_2m_f,
humidity = relative_humidity_2m_percent,
rain = rain_inch,
snow = snowfall_inch,
snow_depth = snow_depth_ft,
cloud_cover = cloud_cover_percent,
wind_speed = wind_speed_10m_mp_h,
wind_gusts = wind_gusts_10m_mp_h
)
View(weather)
# Read weather station lookup table and clean column names
id_lookup <- fread(
"../data/id_lookup.csv",
encoding = "UTF-8",
na.strings = c("", "NA", "NULL"),
fill = TRUE,
data.table = FALSE
) %>%
clean_names()
